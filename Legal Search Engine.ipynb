{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "# Uncomment to see debug logs\n",
    "# logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\n",
    "# logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "from llama_index import VectorStoreIndex, SimpleDirectoryReader, Document, ServiceContext\n",
    "from llama_index.vector_stores import MilvusVectorStore\n",
    "from IPython.display import Markdown, display\n",
    "import textwrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, openai\n",
    "from langchain import OpenAI\n",
    "from llama_index import LLMPredictor\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
    "openai.api_key = \"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\"\n",
    "\n",
    "# set context window\n",
    "context_window = 4096\n",
    "# set number of output tokens\n",
    "num_output = 256\n",
    "# define LLM\n",
    "llm_predictor = LLMPredictor(llm=OpenAI(\n",
    "    temperature=0, \n",
    "    model_name=\"text-davinci-003\", \n",
    "    max_tokens=num_output)\n",
    ")\n",
    "\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    llm_predictor=llm_predictor, \n",
    "    context_window=context_window,\n",
    "    num_output=num_output,\n",
    ")\n",
    "\n",
    "\n",
    "# from llama_index.prompts.prompts import SimpleInputPrompt\n",
    "\n",
    "# system_prompt = \"\"\"<|SYSTEM|># StableLM Tuned (Alpha version)\n",
    "# - StableLM is a helpful and harmless open-source AI language model developed by StabilityAI.\n",
    "# - StableLM is excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n",
    "# - StableLM is more than just an information source, StableLM is also able to write poetry, short stories, and make jokes.\n",
    "# - StableLM will refuse to participate in anything that could harm a human.\n",
    "# \"\"\" \n",
    "\n",
    "# # This will wrap the default prompts that are internal to llama-index\n",
    "# query_wrapper_prompt = SimpleInputPrompt(\"<|USER|>{query_str}<|ASSISTANT|>\")\n",
    "\n",
    "# import torch\n",
    "# from llama_index.llm_predictor import HuggingFaceLLMPredictor\n",
    "# stablelm_predictor = HuggingFaceLLMPredictor(\n",
    "#     max_input_size=4096, \n",
    "#     max_new_tokens=256,\n",
    "#     generate_kwargs={\"temperature\": 0.7, \"do_sample\": False},\n",
    "#     system_prompt=system_prompt,\n",
    "#     query_wrapper_prompt=query_wrapper_prompt,\n",
    "#     tokenizer_name=\"StabilityAI/stablelm-tuned-alpha-3b\",\n",
    "#     model_name=\"StabilityAI/stablelm-tuned-alpha-3b\",\n",
    "#     device_map=\"auto\",\n",
    "#     stopping_ids=[50278, 50279, 50277, 1, 0],\n",
    "#     tokenizer_kwargs={\"max_length\": 4096},\n",
    "#     offload_folder = './offload/'\n",
    "#     # uncomment this if using CUDA to reduce memory usage\n",
    "#     # model_kwargs={\"torch_dtype\": torch.float16}\n",
    "# )\n",
    "# service_context = ServiceContext.from_defaults(\n",
    "#     chunk_size=1024, \n",
    "#     llm_predictor=stablelm_predictor\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document ID: d78c7d6f-0173-4a62-8360-a4aa61d50daa Document Hash: 508e424c6798cc07c09155d5b3169091f398f1a0646566fdcceb1af69688afe8\n"
     ]
    }
   ],
   "source": [
    "documents = SimpleDirectoryReader('data').load_data()\n",
    "print('Document ID:', documents[0].doc_id, 'Document Hash:', documents[0].doc_hash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an index over the documnts\n",
    "from llama_index.storage.storage_context import StorageContext\n",
    "\n",
    "\n",
    "vector_store = MilvusVectorStore(overwrite=True)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "index = VectorStoreIndex.from_documents(documents, service_context = service_context, storage_context=storage_context )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1552 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " On the death of a member, the survivor or survivors where the member was a joint holder, and his\n",
      "nominee or nominees or legal representatives where he was a sole holder, shall be the only persons\n",
      "recognised by the Company as having any title to his interest in the shares. Nothing in Article 17\n",
      "(i) shall release the estate of a deceased joint holder from any liability in respect of any share\n",
      "which had been jointly held by him with other persons.\n"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"What happens on death of a member?\")\n",
    "print(textwrap.fill(str(response), 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
